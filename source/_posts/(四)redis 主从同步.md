---
title: (四)redis 主从同步
date: 2022-11-11 08:04:07
index_img: /img/redis.png
categories:
  - redis
tags:
  - redis
---

### redis 如何保证高可用

对于 redis 来说高可用有两层含义

1. 异常宕机时数据尽量不丢失
2. 服务尽量少中断

对于第 1 点来说,redis 通过 `AOF` 和 `RDB` 两种不同的持久化方式来保证数据尽量不丢失

对于第 2 点来说,如果一个节点挂掉了,想要中断服务的时间尽可能短,最简单的办法就是部署多台实例; 这样某个节点挂掉之后还能有其他节点继续对外提供服务

对于第 2 点,如何保证每个节点之间的数据一致性,又成了难题

#### redis 的读写分离

如果所有节点之间平行对外提供服务,即每个节点都能处理读和写请求; 对于读请求来说是幂等的,所有节点都可以提供读服务,这个不需要额外考虑

对于写请求来说,如果客户端 3 个写请求分别被发到了 3 个不同的节点上,对于客户端来说,后续的查询无论命中了哪个节点,都是脏数据

redis 采用读写分离避免上述情况发生: 读请求从库和主库都能处理响应; 写请求只有主库能够处理,在主库处理完成后同步给其他从库保证数据的 **最终一致性**

可以看到,redis 的读写分离只能保证 **最终一致性**, 不是 **强一致性**

![img.png](https://tva1.sinaimg.cn/large/008vK57jgy1h80wk46gmkj30im067gmt.jpg)

### redis 主从同步

启动 redis 实例时,可以通过 `replaceof` 或者 `slaveof 5.0 之前` 指令形成主从关系,之后会按照 3 个过程进行第一次主从同步

1. 主从之间建立连接,准备开始同步
2. 主库将数据发送给从库
3. 主库将第 2 步期间新进来的写请求发送个从库

![img_1.png](https://tva1.sinaimg.cn/large/008vK57jgy1h80wka46xej30j208fq59.jpg)

#### 建立连接阶段

从库向主库发送指令 `psync` 表示即将进行主从同步,参数包含 `主库的实例 id` 和 `同步进度偏移量` 
`?` 由于第一次进行主从同步,此时从库并不知道主库的实例 id; `-1` 表示第一次进行全量同步

当主库接收到从库的 `psync` 指令后,会通过 `fullsync` 指令响应给从库,并且在响应里面带上自己的 `主库实例 id` 和 `当前的复制进度`

#### 全量同步阶段

在完成握手建立连接之后,主库会把当前的所有数据全部发送给从库; 从库接收到数据后在本地完成数据加载; 这个阶段使用 `RDB` 方式进行数据导入

1. 主库开始全量同步,执行 `bgsave` 命令 `fork` 一个子线程开始生成全量 `RDB` 文件
2. 接着将这个全量 `RDB` 文件发送给从库
3. 从库接收到主库的全量 `RDB` 文件之后,直接清空本地的数据; 使用主库的全量 `RDB` 文件导入到本地的数据库; 避免留下之前的脏数据

根据 `RDB` 快照的生成过程,在主从同步时,主库的主线程仍然能够处理请求,自然也会接收到新的写请求

为了保证这些写请求在从库上不会丢失; 主库会把 `RDB` 期间的写请求全部写入 `RDB` 缓冲区,即 `replication buffer`

当主库将全量 `RDB` 文件发送完成之后,再把这个增量 `RDB` 文件继续发送个从库; 从库将这两个 `RDB` 文件的数据应用到本地的数据库之后,就完成了一次全量同步

#### 级联部署

可以看到在全量同步的过程当中,有两个明显的性能问题

1. 主库生成全量的 `RDB` 文件
2. 主库将全量的 `RDB` 文件通过网络发送给从库

根据之前说的,生成 `RDB` 快照时,即使是通过 `fork` 子线程的方式避免阻塞主线程; 但是如果全量数据过大的话,`fork` 操作本身也会阻塞主线程

而且发送过大的全量 `RDB` 文件,也会消耗大量的网络资源,导致正常的读写请求无法进入主库

为了解决全量同步过程当中主库压力过大的问题,redis 引入了 `级联部署` 的方式,即 `主-从-从` 模式

在之前的主从结构当中,所有的从库都是直接和主库相连,这样对于同一份 `RDB` 快照来说,就要发给所有的从节点

在级联部署当中,可以手动选择一个从库用于级联部分其他的从库,再让这个从库老大成为主库的从库

![img_2.png](https://tva1.sinaimg.cn/large/008vK57jgy1h80wkfiie9j30j809o766.jpg)

这样,就将原来的 `1 主 4 从` 变成了 `1 主 2 从`; 主库在做全量同步的时候,只需要发送 2 份 `RDB` 快照文件即可,网络压力直接减少到原来的一半

#### 网络断连或者阻塞导致的主从同步失败

一旦主库和从库之间完成了首次全量同步,主库就会通过之前已经握手建立好的连接,持续地向从库发送新的写请求; 这个过程称为 `基于长连接的命令传播`

但是考虑到如果网络在这个过程当中出现了问题,如断连或者网络带宽本身就阻塞了; 主从同步就无法继续通过长连接传播命令; 这样也无法保证整个集群的数据一致性了

1. 对于老版本的 redis (2.8 以前) 网络断连之后,下一次连接恢复时,会进行一次全量同步; 如果网络本身质量不好频繁断连,每次的全量同步会造成非常大的性能问题
2. redis 2.8 以后引入了 `增量同步` 技术; 即网络重连之后,不进行全量同步,而是会把主库在断连期间接收到写请求发送给从库

增量同步技术实现的关键点,就是之前介绍的全量同步过程中的第三个阶段,有个 `replication backlog` 缓冲区

当全量同步在生成全量 `RDB` 和发送全量 `RDB` 的过程中,会把主库新接收到的写请求记录在这个 `replication buffer` 当中,当全量 `RDB` 文件发送完毕后,再把这些增量数据全部发送给从库

看一看这个 `replication backlog (复制积压缓冲区)` 的具体构成:

![img_3.png](https://tva1.sinaimg.cn/large/008vK57jgy1h80wkksknij30j204zwgb.jpg)

与 mysql 的 `redo log` 非常类似,`replication backlog` 也是一个环形缓冲区

有两个指针,记录主库写入的位置 `write_pos` 和从库读的位置 `read_pos`,这两个指针中间的差值就成为主从同步的偏移量 `offset` 

需要区分下: `replication buffer` 和 `replication backlog`

1. `buffer` 在每个从库上都有一份,用于接收增量同步进行时的数据
2. `backlog` 只在主库上有一份记录,所有的从库共用这一份 `backlog` 缓冲

回顾一下第一次全量同步的第 1 步,从库向主库发送 `psync ? -1` 指令,其中 `offset=-1` 表示进行一次全量同步

对于第一次全量同步,主库的写入点和从库的读取点是一致的,随着服务的持续运转,持续进行增量主从同步,主库的写入点和从库的读取点都会向后移动,正常来说,这两个指针的偏移量基本相等

如果在某个时刻网络发生了断连,导致主从同步中断,在网络恢复之后; 从库会向主库发送 `psync 主库id offset` 的指令,将自己的读取点告诉给主库

此时主库会判断当前从库的读取点和自己的写入点之间的差距,此时主库会把这中间差距的写入请求重新发送给从库; 由此继续进行增量的同步

![img_4.png](https://tva1.sinaimg.cn/large/008vK57jgy1h80wkp7ytwj30j709omz8.jpg)

#### replication backlog 覆盖写的问题

由于 `replication backlog` 是一个环形的缓冲区,如果网络断连或者主从延迟时间过长,会导致主库的写入点位置超过了从库的读入点,覆盖了部分还没有同步到从库的数据; 导致数据的不一致问题

实际上在配置 `复制积压缓冲区` 的大小时,需要考虑到主库写入速度和网络传输速度; 而且为了应对一些突发状况,实际场景下还要把这个大小扩大一倍

例如当前主库每秒写入 `2000` 条记录,每条记录大小为 `2KB`, 网络每秒能够传输 `1000` 条记录

此时 `replication buffer` 缓冲需要保存 `1000` 条记录,大小至少需要 `2MB`,实际上 `replication backlog` 的大小就需要设置为 `buffer` 的两倍以上,避免覆盖写

但是如果主库压力特别快,写入速度远超于从库同步的进度,再大的 `backlog` 也都将被主库写满; 所以一味地增加 `backlog` 的大小,属于治标不治本的方案