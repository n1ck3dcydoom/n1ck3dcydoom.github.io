---
title: (九)redis 性能分析
date: 2022-11-12 12:25:07
index_img: /img/redis.png
categories:
  - redis
tags:
  - redis
---

### redis 可能存在的性能问题

redis 在运行时,要和非常多的对象进行交互,有些交互在内存里面完成,非常快; 而有些交互则通过网络或者磁盘进行,相对较慢

* 客户端: 通过网络交互,客户端发起增删改查请求,redis 处理这些请求
* 持久化: 通过磁盘交互,redis 有 `AOF` 日志和 `RDB` 快照两种方式进行持久化,都需要最终完成落盘操作
* 主从节点: 通过网络交互,包括哨兵连接,集群节点的 `gossip` 协议交互,主从同步,主从切换,哈希槽迁移

#### redis 和客户端的交互

客户端和 redis 的连接使用了 IO 多路复用机制,令网络 IO 线程专门处理网络请求的建立和响应,而数据的处理则由主线程完成,使得网络 IO 不会阻塞主线程的计算

对于主线程的计算操作,与 mysql 类似,任何大数据操作,大事务等等都会导致实例的性能下降

同样的对于 redis 来说, `集合全量查询` 和 `集合大量数据的聚合操作` 都会显著影响 redis 性能

`keys *` 查询所有 `key` 将会导致 redis 扫描整个数据库,非常影响性能

对数据的增删改,redis 不同的数据结构有些数据的增删很方便,有些数据查询很方便

试想一下,如果对一个大 `key` 进行删除操作,将会引起大量内存释放; 同时操作系统为了避免内存碎片的产生,在有内存释放的地方就会有内存的整理和管理; 这也将导致 redis 主线程的阻塞

同理,清空一个 redis 实例的数据库,也将导致全量数据的删除,也是性能的阻塞点之一

#### redis 和磁盘的交互

在之前介绍持久化的时候已经说过,redis 为了避免写磁盘操作阻塞主线程,无论是在 `AOF` 还是 `RDB` 的过程当中,都是通过 `fork` 一个专门的后台子线程进行持久化操作,这样主线程可以继续处理外部的请求

然而在做 `AOF` 的时候,根据不同的配置决定了不同的刷盘时机,对于 `实时写盘` 的策略,redis 每次在写入 `AOF` 日之后,都将调用 `fsync()` 进行刷盘,这是的磁盘操作就会影响主线程的性能

#### redis 主从节点之间的交互

首次主从同步的时候,从节点会清空本地的所有数据,这就是第一点提到的 **删除大批量数据** 会阻塞整体实例的性能

而且当从节点接收到主节点的全量 `RDB` 数据之后,会把数据加载到自己的内存当中,如果 `RDB` 文件过大,加载的过程也就越慢

#### redis 集群节点之间的交互

当有集群节点因为负载均衡正在进行哈希槽迁移时,如果哈希槽里面有 `bigkey` 的存在,也将导致整个迁移过程变慢,影响到节点的性能

### 异步执行减少阻塞导致的性能问题

1. 读写请求明显无法通过异步执行,读请求是 redis 的关键路径,只能同步进行; 所以对于集合的全量查询,或者聚合操作,都不可避免的会导致性能问题
2. 删除请求,删除请求并不需要客户端同步等待删除结果,所以可以通过后台子线程处理删除操作; 所以对于 `bigkey` 的删除或者清空数据的操作也可以交给后台子线程处理
3. 写 `AOF` 很明显是异步操作,不阻塞主线程
4. 主从同步,从节点加载 `RDB` 快照文件,简单来说其实这里就是读数据,明显也是关键路径只能交给从节点的主线程处理

### redis 的异步执行机制

当一个 redis 实例启动后,主线程会首先创建出 3 个子线程,分别用于 `AOF` 或者 `RDB` 的持久化操作, 键值对的删除操作,以及文件关闭????

主线程通过维护一个任务链表与子线程进行交互,主线程将接收到的删除请求和清空数据等请求,封装为一个任务并放入队列然后返还给客户端表示完成

子线程再等到 CPU 资源分配后,就会从队列里面取出没有处理的任务,开始执行对应的操作

![img.png](https://tva1.sinaimg.cn/large/008vK57jgy1h82ervmomyj30j408stap.jpg)

### CPU 的架构也会影响 redis 的性能

明确一点, **在多 CPU 架构上,应用程序可以运行在不同的处理器上**, 也就是说 redis 可以先在物理核 1 上运行一段时间后,再被调度到物理核 2 上继续运行

由于每个物理核之间的 `私有缓存 L1 和 L3` 是不会共享给其他核心的,当 redis 被调度到另一个物理核运行时,在进行内存访问就需要通过总线访问之前的物理核; 这种跨核心访问内存的行为被称为 `远端访问` ,很明显 `远端访问` 会增加应用处理的延迟

redis 在执行的时候,会记录自身的 `运行时信息` ,还会把访问最为频繁的数据缓存到物理核心的 `L1 和 L2` 缓存上

当发生物理核心调度之后,redis 的 `运行时数据` 就需要重新加载到新的物理核心上,同时之前缓存在 `L1 和 L2` 上的数据也将失效,重新加载到新的物理核心缓存里也会导致应用的运行延迟增大

### CPU 的上下文切换

多核 CPU 架构下, CPU 核心的调度被称为 `上下文切换` 这是引起 redis 性能问题的罪魁祸首

可以通过 `taskset` 指令,将 redis 实例和物理核心进行绑定,避免发生频繁的 `上下文切换`

需要注意的是,并不是说随便将 redis 实例与一个物理核心进行绑定就好了

对于 redis 的网络交互,依赖于操作系统的网络中断程序,当有数据进入网卡硬件后,网络中断层序会把数据从网卡的缓冲区读出并写入内核的缓冲区,然后发出对应的事件

内核的 `epoll` 机制接收到事件后,便会通知 redis 的 IO 线程,这样 redis 的 IO 线程会从内核的缓冲区读出数据,并写入 redis 的用户态缓冲区

如果说网卡的网络中断程序和 redis 实例并不处于同一个物理核心; 此时 redis 从内核态缓冲区读出数据就属于 `远端访问` 这会耗费更多的时间

![img_1.png](https://tva1.sinaimg.cn/large/008vK57jgy1h82es23ug6j30j60bkdjl.jpg)

![img_2.png](https://tva1.sinaimg.cn/large/008vK57jgy1h82es78241j30iu07kq41.jpg)

最好的办法就是把网络中断处理程序和 redis 的实例绑定到同一个物理核心上,避免 `远端访问` 引起的性能问题

![img_3.png](https://tva1.sinaimg.cn/large/008vK57jgy1h82esbqjdaj30j009ijt6.jpg)

### 绑定核心的风险

需要注意的是, `物理核心` 里面还会有 `逻辑核心` ,如果把 redis 实例和 `逻辑核心` 进行了绑定; 这样 redis 在做持久化是 `fork` 的子线程很有可能会与主线程进行资源竞争

所以最好的办法就是把 redis 实例和 `物理核心` 进行绑定,这样可以共用 `物理核心` 下面的所有 `逻辑核心` ,让主线程和子线程分别使用不同的 `逻辑核心` 处理,减少对 CPU 资源的竞争

通过 `lscpu` 指令查看 cpu 核心情况,再通过 `taskset -c xxx,xxx,xxx ./redis-server` 将 redis 实例和某些逻辑核心进行绑定; 要求这些逻辑核心属于同一个物理核心

