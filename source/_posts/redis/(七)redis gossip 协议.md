---
title: (七)redis gossip 协议
date: 2022-11-11 23:34:12
index_img: /img/redis.png
categories:
  - redis
tags:
  - redis
---

### gossip 协议

`Gossip` 协议保证了在一个连通的有界网络当中,每个节点都与其他节点随机通信,当过一段时间后,整个集群的所有节点状态变为一致性的

redis 使用 `Gossip` 协议主要包含以下几个命令:

1. MEET: 已有集群的节点会向新添加的节点发出邀请,使之加入集群内部
2. PING: 每个节点每秒会随机向其他节点发送心跳包,包含当前节点的哈希槽,ip 地址,端口,当前状态以及最后一次通信的时间等信息

毕竟是随机发送,考虑极端情况下,有些节点一直没有被随机选中,这就会导致这些节点一直没有被同步过状态

redis 使用一个配置项 `cluster-node-timeout` 限制最久没有回复 `PONG` 指令的节点,一旦发现自己的节点列表里面有超过 `cluster-node-timeout` 的节点,会立即给这个节点发送 `PING` 指令以同步集群状态

3. PONG: 当节点收到来自其他节点的 `PING` 命令后,会向其回复 `PONG` 消息,消息内容同样包含 `PING` 消息的内容
4. FAIL: 当节点 A `PING` 不通节点 B 时,节点 A 会向集群内其他节点 `广播` 这条消息,其他节点在接收到后会将节点 B 标记为下线

这样在通过一定时间之后,集群当中每个实例都能接收到其他所有实例维护的哈希槽信息,相当于每个实例都保存了其他所有实例的信息

### gossip 协议故障检测

每个节点在发送 `PING` 消息里面都会带上当前节点的状态: `正常运行`, `疑似下线`, `已下线`

1. 当节点 A 得到节点 B 的 `PING` 消息,里面告知节点 C 进入了 `疑似下线` 状态; 此时节点 A 会在自己维护的节点列表里面找到节点 C 的结构体,并将节点 B 的疑似下线消息保存到节点 C
   结构体的 `fail_reports` 链表里
2. 并且节点 A 也会想其它节点传播 `PING` 消息,里面报告了节点 C 进入了 `疑似下线` 状态
3. 如果超过半数以上的节点都认为节点 C `疑似下线`,那么最先将节点 C 标记为 `疑似下线` 的节点 B 会向其他所有节点 `广播` `FAIL` 消息
4. 收到 `FAIL` 消息的节点立即更新自己维护的其他节点状态,将节点 C 标记为 `已下线`

#### gossip 和广播

`gossip` 依靠的是节点之间的互相随机传播,当有非常紧急的消息需要发送时,就需要通过 `广播` 的形式通知其他 `所有` 节点

### 集群节点如何通过 gossip 协议通信

#### 什么时候发送心跳包

redis 节点会记录向 **每一个** 节点最后一次发送 `PING` 和收到 `PONG` 的时间; 然后在节点上有一个定时任务按照一定规则选出一些节点发送 `PING` 心跳包

* 每次定时任务向所有其他未知节点(通常是新加入集群的节点,或者说下线后恢复上线的节点)发送 `MEET` 或者 `PING`
* 每秒从所有已知节点当中选出若干个节点,向其中上一次发送 `PING` 最旧的节点发送 `PING`
* 每次定时任务向收到 `PONG` 超过 `timeout/2` 的节点发送 `PING` 节点
* 每次收到 `MEET` 或者 `PING` 之后,立即回复 `PONG` 

#### 发送的心跳包内容

1. Header:
   * 当前节点维护的哈希槽信息
   * 当前节点的运行状态(正常运行,疑似下线,已下线)
   * 当前节点的主从状态
   * 当前节点的 ip 和端口信息
2. Body
   * 发送者已知节点的 `ping_sent` 和 `pong_received` 信息
   * 发送者已知节点的 ip 和端口信息
   * 发送者已知节点的状态信息(正常运行,疑似下线,已下线)

#### 如何处理心跳

1. 新节点加入时
   * 向起他任意一个节点发送 `MEET`
   * 从 `PONG` 里面得到对方节点已知的其他节点信息将这些节点加入自己的未知节点列表
   * 循环上述步骤,直到所有节点都已经已知,完成新节点加入集群

![img.png](https://tva1.sinaimg.cn/large/008vK57jgy1h8264vk2nij30h30ctmz2.jpg)

2. `slot` 信息
   * 接收消息的节点判断发送消息节点所维护的 `slot` 信息与本地记录是否相同
   * 如果相同,则不做任何处理
   * 如果不相同,判断 `currentEpoch` 谁更大
     * 如果发送方 `currentEpoch` 更大,则将发送方的 `slot` 信息更新到本地
     * 如果接收方 `currentEpoch` 更大,则发送 `UPDATE` 给发送方,让发送方更新 `slot` 新

3. 主从信息
   * 当接收方发现发送方的主从关系发生变更时,更新本地的记录

4. 故障发现
   * 超过超时时间没有回复 `PONG` 的节点,会被发送 `PING` 的节点标记为 `疑似下线`
   * `疑似下线` 消息会随着 `gossip` 协议在集群中传播开来
   * 每个节点收到 `PING` 之后,会检测里面对其他节点的状态信息,如果发现有 `疑似下线` 的节点,则在本地维护一个下线状态投票记录
   * 当某个节点维护的下线状态投票记录里面,对一个 `疑似下线` 节点的标记达到大多数时,当前节点就会向集群传播 `FAIL` 消息
   * 其他节点接收到 `FAIL` 消息后,立马把对应的 `疑似下线` 的节点标记为 `已下线`

### 故障恢复

当一个节点所属的 `master` 节点发生故障后,会尝试进行 `Fail-over 选主` ,由于挂掉的 `master` 可能有多个,所以需要使用类似于 `Raft` 的协议来保证最终一致性

#### currentEpoch - epoch(纪元)

`currentEpoch` 表明了集群中状态变更的迭代版本号,每个节点都会记录自己当前的版本号 

当集群节点创建时,无论是 `master` 还是 `slave` 都将自己的版本号 `currentEpoch` 设置为 0

当节点接收到 `PING` 时,会判断发送者的 `currentEpoch` 和当前自己的 `currentEpoch`

* 如果发送方的 `currentEpoch` 更大,则会将自己的版本号更新为发送方的版本号
* 如果自己的 `currentEpoch`, 则会向发送方回复 `UPDATE` 要求其更新 `currentEpoch` 为自己的 `currentEpoch`

经过 `gossip` 协议传播一段时间后,整个集群的 `currentEpoch` 最终将保持一致

当集群的状态发生改变,通常是发生主从切换即 `master` 节点的故障转移时

`slave` 节点为了执行 `Fail-over 选主` 操作而征求其他节点的同意时,会将自己的 `currentEpoch + 1`,此时这个 `currentEpoch` 时整个集群当中最大的值

* 发起 `Fail-over 选主` 的 `slave` 节点进行广播,这里的广播消息其他 `slave` 节点不会响应,只有其他 `master` 才会响应这条广播消息
* 对于一个 `master` 来说,可能在一个时间段内有多个 `slave` 尝试进行 `Fail-over 选主` 拉票,此时的 `master` 只会为第一个拉票请求头赞成票; 剩下的其他拉票请求都头反对票
* 当尝试 `Fail-over 选主` 的 `slave` 节点收集到所有 `master` 节点的投票信息后,判断有没有超过 `大多数`
* 如果超过了大多数节点,则发送广播消息,告知其他节点自己成为了新的 `master` 节点
* 此时其他所有节点的 `currentEpoch` 值由于都小于新的 `master` 节点的 `currentEpoch` 值,都会在收到广播消息后,更新自己的 `currentEpoch` 为最新值; 在经过一段时间的 `gossip` 之后,整个集群的状态再次恢复一致

#### configEpoch - epoch(纪元)

与 `currentEpoch` 类似,只不过 `configEpoch` 只在节点维护的哈希槽发生变化时才会改变