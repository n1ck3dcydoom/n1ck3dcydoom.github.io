---
title: (十四)mysql 如何保证高可用
date: 2022-11-07 09:27:38
index_img: /img/mysql.png
categories:
  - mysql
tags:
  - 数据库
---

### 双 M 结构

双 M 主从切换是目前比较常用的结构,这种结构使得数据库之间互相进行同步,通过属性 `readonly` 来决定逻辑上的主库

![img.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7wbhy4xwmj30g6065jsl.jpg)

### 主从延迟

mysql 在做主从同步的时候,有几个关键的时间点如下:

1. 主库执行完成一个事务,写入 `binlog` 这个时间点记为 `T1`
2. 主库通过 `dump_thread` 将 `binlog` 发送给备库,备库通过 `io_thread` 将接收到的 `binlog` 写入中转日志 `relay log` 这个时间点记为 `T2`
3. 备库通过 `sql_thread` 线程消费中转日志 `relay log` 完成当前事务的时间点记为 `T3`

那么一个事务从主库到备库的 **主从延迟** 可以通过 `T1-T3` 得到

`binlog` 里每个事务有个时间段字段,这是主库执行事务写入 `binlog` 的时间,备库在做同步的时候取出这个字段和自己的服务器本地时间作对比,计算出主从延迟的具体值即为 `T3-T1`

如果主从库之间的系统时间不一致,备库每次同步的时候,都会向主库执行 `select unix_timesatmp()` 来获取主库的系统时间,在计算主从延迟的时候也会把这个时间加入一起计算

通常来说,在网络正常的情况下, `T2-T1` 的值非常小; 即主从延迟通常体现在备库太慢上,最主要的原因就是 **备库消费中转日志的速率小于主库生产 binlog 的速率** 

### 主从延迟的原因

#### 备库的机器性能比主库差

相同的 sql 语句可能因为备库的机器性能较差从而导致备库执行的时间长于主库执行的时间,最终引发主从延迟

现在几乎都是主从的机器配置保持一样,采用对称配置,避免发生主从切换时,备库的性能问题拖垮整个服务

#### 备库压力过大

在对称部署之后,仍然有主从延迟,还有个很常见的原因就是,备库有很多离线的计算任务

通常主库对外提供服务,而备库只读,所以很多时候会把一些分析任务放到备库里面跑,结果反而占用了备库大量的 CPU 资源,最终影响到主从同步,导致主从延迟发生

解决方案可以分为以下几种:

1. 一主多从: 使用多个备库一起分担这种需要耗费大量 CPU 资源的离线读任务
2. 将 `binlog` 直接输出到外部系统,让外部系统直接提供解析 `binlog` 的能力

#### 大事务

即使配置了一主多从,保证备库的压力不会超过主库,仍然还是可能发生主从延迟

如果说主库上运行了一个大事务,长达 `10min` ,那么备库在同步执行这个大事务的时候很有可能会导致主从延迟 `10min` 以上

最常见的场景就是一次性使用 `delete` 语句删除大量数据,这是个典型的大事务; 包括一次性归档大量数据等

最好是控制单次删除数据的量,分成多批次删除,降低事务的处理时间

另一种常见的大事务场景就是对一张大表做 `DDL` 操作

#### 从库的并行复制能力

如果一个从库消费中转日志 `relay log` 的速率小于主库生产 `binlog` 的速率,很有可能永远都追不上主库,导致长时间的主从延迟,如下图所示

![img_4.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7wbi9bictj30ul0jjdnk.jpg)

在 mysql 5.6 版本之前,从库的 `sql_thread` 只支持单线程,因此在主库并发高的情况下,会导致严重的主从延迟问题

5.6 之后引入了多线程模型,用于解决单线程消费中转日志 `relay log` 过慢的场景

类似于 `Netty` 或者 `Redis` 的网络模型,通常都是将接受网络请求的线程设为 1 个,而处理网络请求的线程设为若干个; 如果直接由若干个处理请求并发消费 `relay log` 每个线程之间都无法知道哪些日志是被消费过的,哪些日志是没有消费过的

![img_5.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7wbih0idbj30rs0jaju0.jpg)

这里面的 `worker` 数量也不能过大,因为在读写分离场景下,从库还需要提供查询能力,如果仅仅由主从同步就将全部 CPU 资源占用了的话,从库的读取能力将大幅下降

mysql 在将 `relay log` 分发给不同的 `worker` 时需要满足以下两个要求

1. 不能造成覆盖更新,也就是更新同一行数据的多个事务,必须由一个 `worker` 顺序执行,否则将会出现其他 `worker` 的更新丢失或者覆盖当前数据
2. 同一个事务不能再拆分

#### 按表分发策略

如果说一个两个事物分别更新两个不同的表,那么可以认为是这两个事务互不影响,可以由两个 `worker` 并行执行; 但是如果事务涉及到了跨表操作,还是需要把涉及到的多张表放到一个 `worker` 执行

![img_6.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7wbin3yf4j30dl0edabx.jpg)

可以看到每个 `workder` 都维护了一个 hash 表,里面维护的是当前 `worker` 的队列里面的事务所涉及到的表, `key` 是表名, `value` 是有多少个事务涉及到

当有新的事务被分配给 `worker` 时,事务里面涉及到的表就会被添加到对应的 hash 表里面, `worker` 执行完成之后就会把对应的 hash 记录移除掉

图中 `hash_table 1` 表示当前 `worker` 的工作队列里面有 4 个事务涉及到 `db1.t1` 表; 有 1 个事务涉及到 `db1.t2` 表

`hash_table 2` 表示当前 `worker` 的工作队列里面有 1 个事务涉及到 `db1.t3` 表

假设分发者从 `relay log` 日志里面消费到一个新的事务 T,这个事务的修改涉及到 `t1 和 t3` 表

1. 根据第一个要求,这个事务涉及到的两个表 `t1 和 t3` 需要检查这两张表上是否有其他事务正在执行
2. `t1` 表由 `worker1` 正在执行,检查 `t3` 发现正在由 `worker2` 执行,由于涉及到多表的事务只能由 1 个 `worker` 执行,所以分发者不能将 T 单独分配给 `worker1 或者 worker2` 只能等待下一次轮训
3. 假设 `worker2` 的工作先结束,从 hash 表里面移除了 `t3` 的相关记录,此时分发者发现这个多表事务只有 `worker1` 有涉及到到,就把事务 T 分发给 `worker1`
4. 分发者继续处理中转日志当中的其他事务

对于分发者来说,他的选择策略如下:

1. 如果当前事务跟所有 `worker` 都不冲突,则把它分给最闲的一个 `worker` 处理, 即 hash 表里面的字段最少的
2. 如果当前事务跟多个 `worker` 冲突,则等待下一次轮训,直到有且只有 1 个 `workder` 冲突时,把事务分给当前 `worker`
3. 如果当前事务只有 1 个 `worker` 跟他冲突,则直接分给那个 `worker` 

很明显,对于表请求负载均衡的场景,这个策略可以很好的胜任工作; 但是如果遇到一张热点表,即大量事务的更新都涉及到这一张表,此时就会把所有事务分配给同一个 `worker` ,这个模型就退化成单线程模型

#### 按行分发策略

按行分发策略要求如果两个事务没有更新到相同的行,则他们可以并发执行; 由于要检测事务里面的更新涉及到的具体行信息,显然 `binlog` 只能设置为 `row` 模式,因为 `statement` 模式记录的原始 sql 不会涉及到具体的行

因此 `worker` 维护的 hash 表里面,就必须保存数据行的唯一性特征,这里的唯一性特征,不仅仅是主键 id,还要把表里面的唯一性索引全都考虑进去,避免出现唯一性校验不通过的情况

可以看到按行分发在解析 `binlog` 时,会耗费更多的 cpu 资源,且要求表必须有主键,不得有外键,因为外键上的级联更新不会记录到 `binlog` 里面,会导致冲突检测不准确,还要求 `binlog` 格式必须是 `row`

#### 按库分发策略

实际上,mysql 既没有采用按行分发,也没有采用按表分发,而是 **按库分发**

* 这样在构造 hash 表的时候就会非常快,而且占用的资源少,因为一个 db 实例上的库毕竟不会太多

* 对于 `binlog` 的格式也没有具体要求

### 由于主从延迟,主从切换时应当选择什么样的策略

#### 可靠性优先策略

双 M 部署的结构,具体的主从切换过程如下

1. 从库判断当前与主库的主从延迟时间,如果超过某个值,则不进行出从切换,循环执行当前步骤直到小于某个可以接受的时间值
2. 将主库改为只读状态 `readonly=true`
3. 由于停止了对主库的写入操作,此时主从同步会慢慢地赶上进度,直到完全同步,即主从延迟时间等于 0
4. 将从库的状态改为读写 `readonly=false`,将请求切换到从库上,完成主从切换

![img_1.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7wbivgw6dj30nr079wgm.jpg)

可以看到在 **可靠性优先策略** 的前提下,会有一个不可用的阶段,其长度等于设定的某个可以接受的主从延迟时间值

#### 可用性优先策略

如果非要把这个不可用时间降低到 0,那只能把第 4 步提前执行; 也就是不等待主从完成同步,直接让从库状态变为读写,然后把请求切到从库

这虽然避免了不可用的情况发生,但是会出现比较严重的数据不一致问题

假如表 `t (id, c)` 有两个字段,且主从库里面同时记录了 3 条数据 `(1,1)(2,2)(3,3)`

此时业务正在往主库里面写入两条数据分别是 `insert c=4, insert c=5`

如果此时已经有 `5s` 的主从延迟,当业务对主库写入 `c=4` 之后,发生主从切换,根据可用性优先策略,此时请求会立马切换到从库,即 `c=5` 的数据是向从库写入的

如果此时的 `binlog_format=STATEMENT`

![img_2.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7wbj1adzyj30og0j6q8r.jpg)

根据主键自增的特性,在 `c=4` 写入主库时,插入的主键 `id=4` 实际写入数据 `(4,4)`, 而发生主从切换之后 `c=5` 写入从库时,插入的主键 `id=4` 也等于 4,实际写入的数据是 `(4,5)` 

`STATEMENT` 格式的 `binlog` 记录了原始 sql 语句,即 `insert id=4`

那么在做出从同步时,新的主库接收到 `insert id=4` 由于主键 id 自增的特性,实际写入的数据是 `(5,4)`

而新的从库接收到 `insert id=5` 由于主键 id 自增的特性,实际写入的数据是 `(5,5)` 

可以看到,在 `id=4` 这条数据上,主从切换后产生了数据不一致

如果说将 `binlog` 格式改为 `ROW` 呢,由于 `ROW` 格式的 `binlog` 会记录插入行的所有字段值,那么在最后的同步过程当中,会报错 **主键重复**

![img_3.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7wbj6jomqj30gg08vmzj.jpg)

因为新主库发送的 sql 里面,包含了主键信息,可以理解为 `insert (id,c) value (4,5)`

新从库发送的 sql 也包含了主键信息,可以理解为 `insert (id,c) value (4,4)`

这样两个库都会因为重复的主键 `id=4` 而报错

### 总结

`row` 格式的 `binlog` 在做主从切换的时候,如果有数据不一致的问题很快就能通过报错发现

而 `statement` 格式的 `binlog` 在上述情况下是可以完成主从切换的,数据不一致的问题悄悄地发生了,严重的话可能要很久之后才会发现

所以 `binlog` 配置大多数时候还是以 `row` 格式为主

而且大多数时候数据的可靠性是优先于服务的可用性的,除非有特殊场景基本上都使用 **可靠性优先策略**




































