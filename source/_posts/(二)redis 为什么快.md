---
title: (二)redis 为什么快
date: 2022-11-10 08:38:07
index_img: /img/redis.png
categories:
  - redis
tags:
  - redis
---

### redis 使用单线程

1. 完全基于内存,绝大部分请求都是在内存中计算处理,少量需要持久化的操作才会涉及到写磁盘
2. 数据结构是专门设计的,操作简单
3. 单线程,省去了线程间的上下文切换和 CPU 消耗,不存在资源竞争,不涉及加锁和释放的操作

这里的单线程指的是 **处理网络 IO 请求和键值对读写(核心工作线程)** 是单线程的,而对于那些需要持久化的操作,例如写日志,异步删除,集群间节点数据同步等是有额外线程执行的

### IO 多路复用

#### select 模型

将所有已连接的 `socket` 都放入一个 **文件描述符集合** 当中,调用 `select` 函数把这个集合拷贝给内核,让内核轮训集合检测是否有网络事件

而内核检测是否有网络事件产生的方式则是通过遍历每个文件描述符,如果有网络事件产生,则将此 `socket` 标记为可读或者可写,遍历完成后把这个集合再拷贝回用户态交给 `select` 函数处理

此时用户态还需要做 **第二次** 遍历,才能找到就绪的 `socket` 并且进行处理

可以看到 `select` 模型有如下特点:

1. 需要遍历 2 次文件描述符集合
2. 发生 2 次用户态到内核态,内核态到用户态的拷贝

而且操作系统在不修改配置的情况下,默认只允许一个进程最大操作 `1024` 个文件描述符

#### poll 模型

几乎与 `select` 一样,仅仅是使用链表来组织 **文件描述符集合** ,突破了 `select` 最大监听数量而已

#### epoll 模型

1. 使用 **红黑树** 存储文件描述符集合
2. 使用 **就绪队列** 存储就绪的文件描述符
3. 每个文件描述符只需要在添加时传入一次,通过事件更改描述符状态

`select, poll` 都只有一个相关函数,而 `epoll` 有 3 个相关函数 

`epoll_create()`

创建一个 `epoll` 实例,其内部主要有两个结构:
* 监听列表: 所有需要监听的文件描述符集合,使用 **红黑树** 存储
* 就绪列表: 监听列表里面已经就绪的文件描述符结合,使用 **队列** 存储

`epoll_ctl()` 

监听文件描述符 `fd` 上发生的 `event` 事件

调用 `epoll_ctl()` 函数会将当前文件描述符 `fd` 添加到 `epoll` 实例的监听列表里面,同时为 `fd` 设置一个回调函数,并且监听指定的 `event` 事件; 当 `fd` 上发生指定事件 `event` 之后,就会调用回调函数将 `fd` 放入 `epoll` 实例的就绪列表里面

`epoll_wati()`

`epoll` 模型的主要处理函数,起作用相当于 `select`,当调用 `epoll_wait()` 时,会返回 `epoll` 实例的就绪列表里面的描述符个数,避免 `select, poll` 每次遍历所有集合元素

#### 水平触发

当监听到文件描述符就绪后,就会触发通知,如果当前文件描述符缓冲区内的数据没有处理完,下次遍历到的时候还会继续发出通知

#### 边缘触发

仅当文件描述符从未就绪变更为就绪时,触发一次通知,且之后不会再次通知; 这要求处理函数必须在通知到来时,将缓冲区内的数据通过循环全部处理完成

边缘触发可以减少 `select` 的调用次数,只不过在一次调用当中需要反复调用多次 `read` 

`epoll` 采用边缘触发的方式,保证每次调用 `epoll_wait()` 都是有效的,因为一次调用必须把缓冲区内的所有数据全部处理完才能返回,否则内核会认为就绪的文件描述符的状态没有发生改变(还有数据没处理完,仍然是就绪状态),从而不再发出后续的通知导致内容丢失

#### redis 的 IO 多路复用

redis 将监听套接字的工作交给内核完成,而内核采用 `epoll` 的机制同事监听多个套接字和管理已就绪的套接字; 一旦有请求到来,通过就绪队列可以快速的找到文件描述符,交给 redis 的 IO 进程处理

这样 redis 就实现了一个 IO 进程复用,处理多个网络请求的效果

![img.png](https://tva1.sinaimg.cn/large/008vK57jgy1h7zrs431btj30io0chq61.jpg)

可以看到,redis 的 IO 进程其实就是在不断的处理 `epoll_create` 创建的 `epoll` 实例的就绪队列,每个就绪的文件描述符因为有 `epoll_wati` 的调用,都记录了监听的事件和对应的回调函数

当 redis 的 IO 进程拿到就绪的文件描述符之后,可以直接调用对应的回调函数进行请求的处理; 并且不会阻塞在某一个请求上,而是可以继续处理队列里下一个就绪的请求

### Linux 零拷贝技术

一次常规的读取数据后写盘操作,其流程如下:

![img.png](https://tva1.sinaimg.cn/large/008vK57jgy1h85k2i93wyj30g808h404.jpg)

1. 用户态发起 `read()` 陷入内核态,内核态发起 IO 请求,读取磁盘中的数据  `(第一次上下文切换)`
2. CPU 将磁盘当中的数据复制到内核缓冲区  `(第一次数据拷贝)`
3. CPU 将内核缓冲区当中的数据复制到用户缓冲区,从内核态切换到用户态  `(第二次上下文切换,第二次数据拷贝)`
4. 进程处理用户缓冲区内的数据
5. 用户态发起 `write()` 陷入内核态,内核态将用户缓冲区的的数据拷贝到内核缓冲区  `(第三次上下文切换,第三次数据拷贝)`
6. 内核态将内核缓冲区内的数据,拷贝到网卡的套接字缓冲区 `(第四次数据拷贝)`
7. 内核态完成 `write()` 调用,返回用户态 `(第四次上下文切换)`

可以看到一次普通的读盘后写盘操作,需要四次上下文切换,四次数据拷贝,而让宝贵的 CPU 去进行数据拷贝这样的阻塞操作,是很难接受的

引入 `DMA` 总线控制器后,CPU 就不再直接参与磁盘的读取和数据拷贝了,而是通过 `DMA` 的完成事件通知异步的将磁盘缓冲区内的数据拷贝到内核缓冲区

![img_1.png](https://tva1.sinaimg.cn/large/008vK57jgy1h85k2p62j6j30id0bzq51.jpg)

对于上下文切换和拷贝次数并没有优化,仅仅是把 CPU 从阻塞的磁盘 IO 中解放出来可以继续做其他计算工作

#### 虚拟内存映射 mmap

Linux 提供了一种内存映射的机制,可以把内核缓冲区所在的地址空间与用户缓冲区所在的地址空间进行一次映射,这个技术使得 `用户态可以直接访问内核缓冲区内的数据`

![img_2.png](https://tva1.sinaimg.cn/large/008vK57jgy1h85k2uq1bfj30ip0caad7.jpg)

虚拟地址映射时,有两次上下文切换 `(用户态调用 write 和 write 调用返回内核态)`,一次 CPU 拷贝,两次 DMA 拷贝

#### sendfile

Linux 2.1 版本以后还提供了 `sendfile` 的方式,可以进一步减少拷贝次数,但问题就是由于 `sendfile` 不经过用户态,所以无法修改数据只能进行数据发送

![img_3.png](https://tva1.sinaimg.cn/large/008vK57jgy1h85k301thaj30hy0bt40s.jpg)

在 `sendfile` 函数调用下,仅有两次上下文切换 `(用户态调用 sendfile 和 sendfile 调用返回用户态)`,一次 CPU 拷贝,两次 DMA 拷贝

#### sendfile + DMA 直接拷贝

Linux 2.4 以后,对 `sendfile` 进行了优化,进一步减少了 CPU 的拷贝次数; `DMA` 通过记录内核缓冲区内的地址和偏移量,直接将数据拷贝到网卡缓冲内

![img_4.png](https://tva1.sinaimg.cn/large/008vK57jgy1h85k34q0uuj30if0bhwgy.jpg)

可以看到本质还是 `sendfile` 所以仍然无法进行数据修改